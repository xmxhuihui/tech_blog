[{"content":"云平台：AutoDL 模型加载工具：Ollama 参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\n下载Ollama 服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\nhttps://ollama.com/download/ollama-linux-amd64.tgz复制到浏览器进行下载 下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。 打开一个terminal，解压下载的文件包：\ntar -C /usr -xzf ollama-linux-amd64.tgz 启用ollama ollama serve 再打开一个terminal，检查一下ollama是否启用 ollama -v 这里可以看到ollama版本，说明启用成功。 拉取模型 这里以deepseek-r1:32b为例进行拉取\nollama pull deepseek-r1:32b 下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。 下载完成以后可以查看一下模型列表：\nollama list 运行模型：\nollama run deepseek-r1:32b 测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小 这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\n","permalink":"http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/","summary":"\u003cp\u003e云平台：AutoDL\n模型加载工具：Ollama\n参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\u003c/p\u003e\n\u003ch2 id=\"下载ollama\"\u003e下载Ollama\u003c/h2\u003e\n\u003cp\u003e服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/download/ollama-linux-amd64.tgz\"\u003ehttps://ollama.com/download/ollama-linux-amd64.tgz\u003c/a\u003e复制到浏览器进行下载\n下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。\n打开一个terminal，解压下载的文件包：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etar -C /usr -xzf ollama-linux-amd64.tgz\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"启用ollama\"\u003e启用ollama\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama serve\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"再打开一个terminal检查一下ollama是否启用\"\u003e再打开一个terminal，检查一下ollama是否启用\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama -v\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这里可以看到ollama版本，说明启用成功。\n\u003cimg alt=\"版本查看\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"拉取模型\"\u003e拉取模型\u003c/h2\u003e\n\u003cp\u003e这里以deepseek-r1:32b为例进行拉取\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama pull deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"拉取模型\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-1.png\"\u003e\n\u003c!-- raw HTML omitted --\u003e下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。\u003c!-- raw HTML omitted --\u003e\n下载完成以后可以查看一下模型列表：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama list\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"模型列表\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e运行模型：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama run deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小\n\u003cimg alt=\"问题测试\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-4.png\"\u003e\n这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\u003c/p\u003e","title":"Full process of deploying deepseek on remote server"},{"content":"云平台：AutoDL 模型加载工具：Ollama 参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\n下载Ollama 服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\nhttps://ollama.com/download/ollama-linux-amd64.tgz复制到浏览器进行下载 下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。 打开一个terminal，解压下载的文件包：\ntar -C /usr -xzf ollama-linux-amd64.tgz 启用ollama ollama serve 再打开一个terminal，检查一下ollama是否启用 ollama -v 这里可以看到ollama版本，说明启用成功。 拉取模型 这里以deepseek-r1:32b为例进行拉取\nollama pull deepseek-r1:32b 下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。 下载完成以后可以查看一下模型列表：\nollama list 运行模型：\nollama run deepseek-r1:32b 测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小 这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\n","permalink":"http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/","summary":"\u003cp\u003e云平台：AutoDL\n模型加载工具：Ollama\n参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\u003c/p\u003e\n\u003ch2 id=\"下载ollama\"\u003e下载Ollama\u003c/h2\u003e\n\u003cp\u003e服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/download/ollama-linux-amd64.tgz\"\u003ehttps://ollama.com/download/ollama-linux-amd64.tgz\u003c/a\u003e复制到浏览器进行下载\n下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。\n打开一个terminal，解压下载的文件包：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etar -C /usr -xzf ollama-linux-amd64.tgz\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"启用ollama\"\u003e启用ollama\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama serve\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"再打开一个terminal检查一下ollama是否启用\"\u003e再打开一个terminal，检查一下ollama是否启用\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama -v\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这里可以看到ollama版本，说明启用成功。\n\u003cimg alt=\"版本查看\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"拉取模型\"\u003e拉取模型\u003c/h2\u003e\n\u003cp\u003e这里以deepseek-r1:32b为例进行拉取\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama pull deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"拉取模型\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-1.png\"\u003e\n\u003c!-- raw HTML omitted --\u003e下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。\u003c!-- raw HTML omitted --\u003e\n下载完成以后可以查看一下模型列表：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama list\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"模型列表\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e运行模型：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama run deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小\n\u003cimg alt=\"问题测试\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-4.png\"\u003e\n这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\u003c/p\u003e","title":"Full process of deploying deepseek on remote server"},{"content":"云平台：AutoDL 模型加载工具：Ollama 参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\n下载Ollama 服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\nhttps://ollama.com/download/ollama-linux-amd64.tgz复制到浏览器进行下载 下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。 打开一个terminal，解压下载的文件包：\ntar -C /usr -xzf ollama-linux-amd64.tgz 启用ollama ollama serve 再打开一个terminal，检查一下ollama是否启用 ollama -v 这里可以看到ollama版本，说明启用成功。 拉取模型 这里以deepseek-r1:32b为例进行拉取\nollama pull deepseek-r1:32b 下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。 下载完成以后可以查看一下模型列表：\nollama list 运行模型：\nollama run deepseek-r1:32b 测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小 这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\n","permalink":"http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/","summary":"\u003cp\u003e云平台：AutoDL\n模型加载工具：Ollama\n参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\u003c/p\u003e\n\u003ch2 id=\"下载ollama\"\u003e下载Ollama\u003c/h2\u003e\n\u003cp\u003e服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/download/ollama-linux-amd64.tgz\"\u003ehttps://ollama.com/download/ollama-linux-amd64.tgz\u003c/a\u003e复制到浏览器进行下载\n下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。\n打开一个terminal，解压下载的文件包：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etar -C /usr -xzf ollama-linux-amd64.tgz\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"启用ollama\"\u003e启用ollama\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama serve\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"再打开一个terminal检查一下ollama是否启用\"\u003e再打开一个terminal，检查一下ollama是否启用\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama -v\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这里可以看到ollama版本，说明启用成功。\n\u003cimg alt=\"版本查看\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"拉取模型\"\u003e拉取模型\u003c/h2\u003e\n\u003cp\u003e这里以deepseek-r1:32b为例进行拉取\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama pull deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"拉取模型\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-1.png\"\u003e\n\u003c!-- raw HTML omitted --\u003e下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。\u003c!-- raw HTML omitted --\u003e\n下载完成以后可以查看一下模型列表：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama list\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"模型列表\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e运行模型：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama run deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小\n\u003cimg alt=\"问题测试\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-4.png\"\u003e\n这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\u003c/p\u003e","title":"Full process of deploying deepseek on remote server"},{"content":"云平台：AutoDL 模型加载工具：Ollama 参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\n下载Ollama 服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\nhttps://ollama.com/download/ollama-linux-amd64.tgz复制到浏览器进行下载 下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。 打开一个terminal，解压下载的文件包：\ntar -C /usr -xzf ollama-linux-amd64.tgz 启用ollama ollama serve 再打开一个terminal，检查一下ollama是否启用 ollama -v 这里可以看到ollama版本，说明启用成功。 拉取模型 这里以deepseek-r1:32b为例进行拉取\nollama pull deepseek-r1:32b 下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。 下载完成以后可以查看一下模型列表：\nollama list 运行模型：\nollama run deepseek-r1:32b 测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小 这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\n","permalink":"http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/","summary":"\u003cp\u003e云平台：AutoDL\n模型加载工具：Ollama\n参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\u003c/p\u003e\n\u003ch2 id=\"下载ollama\"\u003e下载Ollama\u003c/h2\u003e\n\u003cp\u003e服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/download/ollama-linux-amd64.tgz\"\u003ehttps://ollama.com/download/ollama-linux-amd64.tgz\u003c/a\u003e复制到浏览器进行下载\n下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。\n打开一个terminal，解压下载的文件包：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etar -C /usr -xzf ollama-linux-amd64.tgz\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"启用ollama\"\u003e启用ollama\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama serve\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"再打开一个terminal检查一下ollama是否启用\"\u003e再打开一个terminal，检查一下ollama是否启用\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama -v\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这里可以看到ollama版本，说明启用成功。\n\u003cimg alt=\"版本查看\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"拉取模型\"\u003e拉取模型\u003c/h2\u003e\n\u003cp\u003e这里以deepseek-r1:32b为例进行拉取\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama pull deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"拉取模型\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-1.png\"\u003e\n\u003c!-- raw HTML omitted --\u003e下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。\u003c!-- raw HTML omitted --\u003e\n下载完成以后可以查看一下模型列表：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama list\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"模型列表\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e运行模型：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama run deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小\n\u003cimg alt=\"问题测试\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-4.png\"\u003e\n这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\u003c/p\u003e","title":"Full process of deploying deepseek on remote server"},{"content":"云平台：AutoDL 模型加载工具：Ollama 参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\n下载Ollama 服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\nhttps://ollama.com/download/ollama-linux-amd64.tgz复制到浏览器进行下载 下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。 打开一个terminal，解压下载的文件包：\ntar -C /usr -xzf ollama-linux-amd64.tgz 启用ollama ollama serve 再打开一个terminal，检查一下ollama是否启用 ollama -v 这里可以看到ollama版本，说明启用成功。 拉取模型 这里以deepseek-r1:32b为例进行拉取\nollama pull deepseek-r1:32b 下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。 下载完成以后可以查看一下模型列表：\nollama list 运行模型：\nollama run deepseek-r1:32b 测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小 这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\n","permalink":"http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/","summary":"\u003cp\u003e云平台：AutoDL\n模型加载工具：Ollama\n参考：https://github.com/ollama/ollama/blob/main/docs/linux.md\u003c/p\u003e\n\u003ch2 id=\"下载ollama\"\u003e下载Ollama\u003c/h2\u003e\n\u003cp\u003e服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ollama.com/download/ollama-linux-amd64.tgz\"\u003ehttps://ollama.com/download/ollama-linux-amd64.tgz\u003c/a\u003e复制到浏览器进行下载\n下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。\n打开一个terminal，解压下载的文件包：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etar -C /usr -xzf ollama-linux-amd64.tgz\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"启用ollama\"\u003e启用ollama\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama serve\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"再打开一个terminal检查一下ollama是否启用\"\u003e再打开一个terminal，检查一下ollama是否启用\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama -v\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这里可以看到ollama版本，说明启用成功。\n\u003cimg alt=\"版本查看\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"拉取模型\"\u003e拉取模型\u003c/h2\u003e\n\u003cp\u003e这里以deepseek-r1:32b为例进行拉取\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama pull deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"拉取模型\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-1.png\"\u003e\n\u003c!-- raw HTML omitted --\u003e下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。\u003c!-- raw HTML omitted --\u003e\n下载完成以后可以查看一下模型列表：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama list\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"模型列表\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e运行模型：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eollama run deepseek-r1:32b\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小\n\u003cimg alt=\"问题测试\" loading=\"lazy\" src=\"/posts/full-process-of-deploying-deepseek-on-remote-server/image-4.png\"\u003e\n这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧\u003c/p\u003e","title":"Full process of deploying deepseek on remote server"}]