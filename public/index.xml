<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>xmxhuihui Tech Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on xmxhuihui Tech Blog</description>
    <generator>Hugo -- 0.147.5</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 May 2025 11:30:40 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Full process of deploying deepseek on remote server</title>
      <link>http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/</link>
      <pubDate>Mon, 26 May 2025 11:30:40 +0100</pubDate>
      <guid>http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/</guid>
      <description>&lt;p&gt;云平台：AutoDL
模型加载工具：Ollama
参考：https://github.com/ollama/ollama/blob/main/docs/linux.md&lt;/p&gt;
&lt;h2 id=&#34;下载ollama&#34;&gt;下载Ollama&lt;/h2&gt;
&lt;p&gt;服务器上下载ollama比较慢，因此我使用浏览器先下载到本地电脑上。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ollama.com/download/ollama-linux-amd64.tgz&#34;&gt;https://ollama.com/download/ollama-linux-amd64.tgz&lt;/a&gt;复制到浏览器进行下载
下载好以后，使用FileZilla等工具上传到云服务器上，这里我上传到autodl-tmp文件夹下。
打开一个terminal，解压下载的文件包：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tar -C /usr -xzf ollama-linux-amd64.tgz
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;启用ollama&#34;&gt;启用ollama&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama serve
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;再打开一个terminal检查一下ollama是否启用&#34;&gt;再打开一个terminal，检查一下ollama是否启用&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama -v
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里可以看到ollama版本，说明启用成功。
&lt;img alt=&#34;版本查看&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/image.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;拉取模型&#34;&gt;拉取模型&lt;/h2&gt;
&lt;p&gt;这里以deepseek-r1:32b为例进行拉取&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama pull deepseek-r1:32b
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img alt=&#34;拉取模型&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/image-1.png&#34;&gt;
&lt;!-- raw HTML omitted --&gt;下载过程中可能有掉速，Ctrl+C掉以后再重新ollama pull可以继续下载。&lt;!-- raw HTML omitted --&gt;
下载完成以后可以查看一下模型列表：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama list
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img alt=&#34;模型列表&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/image-3.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;运行模型：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama run deepseek-r1:32b
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;测试了一个很常见的，却有许多模型答错的问题：9.11和9.9比大小
&lt;img alt=&#34;问题测试&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/full-process-of-deploying-deepseek-on-remote-server/image-4.png&#34;&gt;
这个模型的链式推理很有人味儿，很有吸引力，这可能就是这个模型受很多人关注和称赞的原因吧&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
